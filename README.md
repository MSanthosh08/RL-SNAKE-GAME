
# RL Snake â€“ Advanced DQN Edition ðŸðŸ¤–

This version of the Snake RL project adds:

- âœ… Double DQN with target network
- âœ… Prioritized Experience Replay (PER)
- âœ… MLP agent using state features
- âœ… CNN agent using pixel observations
- âœ… Save / load models to resume training
- âœ… Live matplotlib plots during training
- âœ… Streamlit dashboard for monitoring training

## Structure

- `game.py` â€” Snake environment with improved reward shaping and state, plus `get_frame()` for CNN.
- `model.py` â€” Linear Q-network + `QTrainer` with Double DQN and target network.
- `agent.py` â€” MLP-based DQN agent using state vector + PER.
- `model_cnn.py` â€” CNN Q-network for pixel input.
- `agent_cnn.py` â€” DQN agent using CNN + PER.
- `train.py` â€” Trains the MLP agent; logs to `training_log.csv`.
- `train_cnn.py` â€” Trains the CNN agent; logs to `training_log_cnn.csv`.
- `dashboard.py` â€” Streamlit dashboard to visualize training logs.
- `requirements.txt` â€” Dependencies.

## Install

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Train (State-Based Agent)

```bash
python train.py
```

This will:
- Open a Pygame window (snake learning)
- Show a live matplotlib plot
- Log metrics into `training_log.csv`

## Train (CNN Pixel-Based Agent)

```bash
python train_cnn.py
```

This uses the `get_frame()` output as CNN input and logs to `training_log_cnn.csv`.

> Note: CNN training is heavier; consider smaller window sizes or fewer games for testing.

## Streamlit Dashboard

In another terminal:

```bash
streamlit run dashboard.py
```

Then open the URL shown in the terminal.  
Youâ€™ll see separate tabs for:
- State-based MLP agent
- CNN agent

Both read from the CSV logs created during training.

## Resume Training

- Models are automatically saved into the `./model` folder.
- On startup, both agents attempt to load the saved weights.
